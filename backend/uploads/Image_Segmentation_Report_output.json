{
    "nodes": [
        {
            "id": "e169ba0fce608265d0e28c19d9724396",
            "level": 0,
            "content": "Technical Report on Image Segmentation using Mask2Former Shayak Bhattacharya ∗ Sagnik Debnath † ∗ Department of Mathematics, IIT Guwahati † Department of Mathematics, IIT Guwahati Email: b.shayak@iitg.ac.in Email: d.sagnik@iitg.ac.in Bhupathi Nikhil Agnihotri ‡ ‡ Department of Mathematics, IIT Guwahati Email: a.bhupathi@iitg.ac.in I.  ABSTRACT This paper explores the use of the Mask2Former model for semantic segmentation. Specifically, addressing the chal- lenges in Indian road datasets under various scenarios. Mask2Former’s mask focus mechanism and multi-scale high- resolution feature enhance segmentation performance. Espe- cially in detecting rare classes. Evaporation studies show the effects of various parameters. on segmentation quality It emphasizes the adaptability of the model to different tasks. We evaluate our approach using metrics such as the average Dice coefficient, F1 score, and Mean IoU. Index Terms —Image Segmentation, Mask2Former, Semantic Segmentation, Deep Learning, Transformer, Ablation Study. II. I NTRODUCTION in computer vision Image segmentation is a basic operation. It assigns each pixel to a class and divides it into meaningful segments. They are used in fields ranging from autonomous driving to medical imaging. This is where accurate, real- time segmentation is important. Traditional methods for image segmentation usually rely on convolutional neural networks (CNNs) for per-pixel classification, but CNNs face challenges in capturing global context due to the limited receiver space. Recent advances use autopilot-based architectures to exploit attention mechanisms. Mask2Former is a universal segmentation model designed to handle multiple semantic, instance, and aggregate segmenta- tion tasks within a single architecture. Key innovations include a masked attention mechanism that focuses only on relevant areas. This improves computational efficiency and accuracy. This paper uses Mask2Former on a large Indian road dataset with diverse environments and rare types. The aim is to demonstrate the flexibility and effectiveness of the model in complex real-world situations. III. A PPROACH  O VERVIEW This approach focuses on optimizing Mask2Former for high-resolution image segmentation tasks. With specific op- timizations for handling unbalanced rare classes in the dataset and also perform Ablation study for hyperparameter tuning A. Dataset Preprocessing The dataset consists of high-resolution images (1920x1080) and corresponding segmented maps. The classes are very unbalanced. Some classes represent thousands of samples while some are rare. pre-processing includes segmentation map analysis. Identification of rare classes and filtering out mislabeled samples. A custom extraction strategy was used. By choosing pictures according to the variety of classes. The existence of rare classes and per-subset size to ensure balanced subsets for model fine-tuning B. Model Architecture Mask2Former’s architecture is based on the Transformer framework. It is optimized for segmentation through multi- scale mask study and high-resolution feature extraction. Masked attention focuses on pixels in the projected mask area. Ignoring unrelated areas and reducing the burden of calculations This targeted approach improves performance and enables fine-grained segmentation in complex scenes. The model makes use of the core feature extractor. Pixel decoder for high-resolution embedding and a Transformer decoder that dynamically adjusts the section approximation... C. Training and Fine-tuning The training uses a pre-trained Mask2Former model. where the encoder and decoder pixels are frozen to preserve the pre- learned representation. Fine-tuning will focus on the Trans- former decoder and the classification level optimized for the current dataset. A custom data pipeline processes the input images into a Mask2Former-compatible format, generating segmentation maps and mask labels for each set. Training parameters include a learning rate of 5e-5, a batch size of 8, and an optimizer Like Adam or AdamW for three epochs. IV. E QUATIONS Mean IoU  =   1 n n X i =1 | A i  ∩ B i | | A i  ∪ B i | (1) F1 Score  =   2  ×  Precision  ×  Recall Precision  +  Recall (2) Precision  = True Positives True Positives  +  False Positives (3) Recall  = True Positives True Positives  +  False Negatives (4) Dice Coefficient  =   2  × | A  ∩ B | | A |  +  | B | (5) V. R ESULTS AND  D ISCUSSION The model’s performance was evaluated using metrics namely mean Dice coefficient, F1 score, and the average IoU across validation images. The segmentation maps generated by Mask2Former demonstrated consistent accuracy, with high precision in rare class segmentation due to the model’s ability to handle high-resolution features. Fig. 1. Sample segmentation results on Indian roadway dataset showing segmentation for diverse classes with a comparison between Ground Truth and Prediction. VI. A BLATION  S TUDY Ablation studies are performed to understand the effects of key model components and parameters. Concealed attention and multi-scale features are the main contributions to the model’s performance. It has a high-resolution feature map, which provides better segmentation accuracy for small objects. The following evaporation figures show the impact of different architectures. Performing Further Ablation studies by varying parameters such as Batch Size and Epochs and changing the Optimizer for Different Parameters, we obtained the Following Results. Fig. 2. Comparison of Training Loss Across Epochs for modified Mask2Former Image Segmentation Model Fig. 3. Mean Dice, Mean F1, and Mean IoU Parameter Scores Across Epochs for Mask2Former Model A. Variation of Number of Epochs We can see the Loss Gradually Reducing and Converging to a value Quickly. If it was Run for a greater number of Epochs, Due to Over-shooting we Loose the Optimal point. The overall increase in Mean Dice, Mean IoU, and Mean F1 scores, although with slight fluctuation reflects a good balance in the model’s segmentation performance. The metric indicates a good overlap between the predicted and actual segmentation areas. The above Data is in Tabulated Format Below. TABLE I S EGMENTATION  P ERFORMANCE  M ETRICS Epoch Mean Dice Mean F1 Mean IoU 1 0.514 0.497 0.416 2 0.548 0.537 0.442 3 0.550 0.535 0.444 4 0.540 0.529 0.435 5 0.552 0.539 0.448 Fig. 4. Variation of Batch Size using Adam Optimizer Comparing Loss factor Fig. 5. Variation of Batch Size using AdamW Optimizer Comparing Loss factor B. Variation of Batch size for different Optimizers We observe the effect of varying batch sizes on the training dynamics of image segmentation models using Adam and AdamW optimizers. Larger batch sizes tend to stabilize train- ing but may slow convergence, while smaller sizes accelerate the convergence. Comparing Adam and AdamW, we know that it is almost invariant changes for a small number of Epochs but by increasing the epochs we may observe differences between Optimizers. VII. C ONCLUSION The Mask2Former model has proven to be effective in universal image segmentation tasks. and adapt to various challenging situations very well in the Indian road dataset. The model’s concealed focus and high-resolution features enable accurate segmentation even in rarefied modes. Future work will focus on further optimizing the Mask2Former architecture to improve small object segmen- tation. and explore the potential of advanced converter back- bones for writing better encoding. A CKNOWLEDGMENT R EFERENCES [1] Cheng, Bowen, Alexander G. Schwing, and Alexander Kirillov.  Per- Pixel Classification is Not All You Need for Semantic Segmentation . In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 4371-4380. 2021. [2] Cheng, Bowen, Ishan Misra, Alexander G. Schwing, and Alexander Kirillov.  Masked-attention Mask Transformer for Universal Image Seg- mentation . In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1290-1299. 2022. [3] Hugging Face. Transfer Learning with Transformers . Available online: https://colab.research.google.com/github/johko/computer-vision- course/blob/main/notebooks [4] He, Kaiming, Georgia Gkioxari, Piotr Doll´ar, and Ross Girshick.  Mask R-CNN . In Proceedings of the IEEE International Conference on Com- puter Vision (ICCV), pp. 2961-2969. 2017. [5] Liu, Ze, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo.  Swin Transformer: Hierarchical Vision Transformer using Shifted Windows . In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pp. 10012-10022. [6] Zhao, Hengshuang, Jianping Shi, Xiaojuan Qi, Xiaogang Wang, and Jiaya Jia.  Pyramid Scene Parsing Network . In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 2881-2890. 2017. [7] Cordts, Marius, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, and Bernt Schiele.  The Cityscapes Dataset for Semantic Urban Scene Understanding . In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 3213-3223. 2016. [8] Varma, M., Subramanian, V., Namboodiri, V., Chandraker, M., and Jawahar, C.V.  IDD: A Dataset for Exploring Problems of Au- tonomous Navigation in Unconstrained Environments . Available on- line: https://idd.insaan.iiit.ac.in/dataset/details/ (accessed [Insert Date Accessed]).",
            "font_size": 100,
            "is_underlined": false
        }
    ],
    "edges": []
}